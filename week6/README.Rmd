---
title: "Lab 6"
author: "Audrey Omidsalar"
date: "10/1/2021"
output:
  html_document:
    toc: yes
    toc_float: yes
    keep_md: yes
  github_document:
  always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('data.table')
library('tidytext')
library('ggplot2')
library('dplyr')
library('tibble')
library('forcats')
library('tidyr')
```

### Read in Data
```{r download data, cache = TRUE}
fn <- "mtsamples.csv"
if (!file.exists(fn))
  download.file("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv", destfile = fn)
mtsamples <- read.csv(fn)
##convert data frame into tibble
mtsamples <- as_tibble(mtsamples)
head(mtsamples)
##piping it to knitr::kable() makes the table look pretty
```
## Question 1
### What specialties do we have?
There are 40 specialties in total within this dataset. The specialties do not seem to be evenly distributed: surgery, cardiovascular, consult, and orthopedic have the most amount of entries.
```{r question1}
mtsamples %>% count(medical_specialty)
ggplot(data = mtsamples) +
  geom_bar(mapping= aes(y = medical_specialty), color = 'darkgreen',fill = 'green3')

```
```{r include = FALSE}
specialties <- mtsamples %>% count(medical_specialty)
#There are `r nrow(specialties)` specialties. Now let's take a look at the distribution
```
## Question 2
### Tokenize the words in the `transcription` column. Count the number of times each token appears. Visualize the top 20 most frequent words.
Not surprisingly, the majority of the top 20 most frequent words are stop words. The one that stands out is the word *patient*.
```{r question2, cache = TRUE}
mtsamples %>% unnest_tokens(token, transcription) %>% count(token, sort = TRUE) %>% top_n(20, n) %>% ggplot(aes(x = n, y = fct_reorder(token, n ))) + 
  geom_col(color = 'darkgreen', fill = 'green3') +
  labs(title = "Top 20 words", y = "", x = "frequency")
```

## Question 3

### Redo visualization but remove stopwords as well as numbers.
Now we can see more terms that are related to this dataset.

```{r question3}
mtsamples %>%
  unnest_tokens(token, transcription) %>%
  count(token, sort = TRUE) %>% 
  anti_join(stop_words, by = c("token" = "word")) %>% 
  filter(!grepl("^[0-9]+$", x = token)) %>%
  top_n(20, n) %>% 
  ggplot(aes(x = n, y = fct_reorder(token, n ))) + 
    geom_col(color = 'darkgreen', fill = 'green3') +
    labs(title = "Top 20 Words, without Stopwords and Numbers", y = "", x = "frequency")
```

## Question 4
### Repeat question 2, but this time tokenize into bi-grams.
A lot of these contain stop-words, so this is not very informative.
```{r question4, cache = TRUE}
mtsamples %>% unnest_ngrams(ngram, transcription, n = 2) %>% 
  count(ngram, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(x = n, y = fct_reorder(ngram, n))) +
  geom_col(color = 'darkgreen', fill = 'green3') +
  labs(title = "Top 20 Bi-grams", y = "", x = "frequency")
```
### How does the result change if you look at tri-grams?
The top bi-gram *the patient* appears in the top two tri-grams. Some of the other top bi-grams (of the, in the, to the) don't show up as frequently because they are likely being used with a variety of words before/after them. These tri-grams are more unique than the list of bi-grams in containing words that are medical-related.
The frequencies between these two graphs is also very different. The top bi-gram has a frequency of over 20,000 while the top tri-gram has a frequency of approximately 6000.
```{r tri-gram, cache = TRUE}
mtsamples %>% unnest_ngrams(ngram, transcription, n = 3) %>% 
  count(ngram, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(x = n, y = fct_reorder(ngram, n))) +
  geom_col(color = 'darkgreen', fill = 'green3') +
  labs(title = "Top 20 Tri-grams", y = "", x = "frequency")
```

## Question 5
### Using the results you got from questions 4. Pick a word and count the words that appears after and before it.
Picking word *patient*
```{r question 5, cache = TRUE}
patient <- mtsamples %>% unnest_ngrams(ngram, transcription, n = 2) %>% 
  separate(ngram, into = c("word1", "word2"), sep = " ") %>%
  select(word1, word2) %>%
  filter(word1 == "patient" | word2 == "patient")
```

```{r question5 - remove stops and make tables}
##Remove stop words and numbers
patient %>%
  filter(word1 == "patient") %>%
  filter(!(word2 %in% stop_words$word) & !grepl("^[0-9]+$", x = word2)) %>%
  count(word2, sort = TRUE) %>%
  top_n(20, n) %>% knitr::kable()
patient %>%
  filter(word2 == "patient") %>%
  filter(!(word1 %in% stop_words$word) & !grepl("^[0-9]+$", word1)) %>%
  count(word1, sort = TRUE) %>%
  top_n(20, n) %>% knitr::kable()
```

## Question 6
### Which words are most used in each of the specialties. you can use group_by() and top_n() from dplyr to have the calculations be done within each specialty. Remember to remove stopwords. How about the 5 most used words?
The table below shows the top 5 words per medical specialty. Some specialties, such as Allergy / Immunology, have ties in the ranking and, therefore, more than five rows are shown.
```{r question 6}
mtsamples %>% group_by(medical_specialty) %>%
  unnest_tokens(token, transcription) %>% 
  count(token, sort = TRUE) %>% 
  anti_join(stop_words, by = c("token" = "word")) %>% 
  filter(!grepl("^[0-9]+$", x = token)) %>%
  top_n(5, n) %>%
  arrange(medical_specialty, desc(n)) %>% knitr::kable()
```

## Question 7
### 
