---
title: "Lab 7"
author: "Audrey Omidsalar"
date: "10/8/2021"
output:
  html_document:
    toc: yes
    toc_float: yes
    keep_md: yes
  github_document:
  always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('data.table')
library('tidytext')
library('dplyr')
library('tibble')
library('forcats')
library('tidyr')
library('httr')
library('xml2')
library('stringr')
```

## Question 1: How many SARS-CoV2 papers?
```{r q1}
# Downloading the website
website <- read_html(x = "https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2")

# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex
numberpapers  <- stringr::str_extract(counts, "[:digit:]+.*[:digit:]")
```

#### There are `r numberpapers` papers about SARS-CoV2 on PubMed

## Question 2: Academic publications on COVID19 and Hawaii

```{r q2}
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db = "pubmed",
    term = "covid19 hawaii",
    retmax = 1000
  )
)

# Extracting the content of the response of GET
ids <- httr::content(query_ids)
```

## Question 3: Get details about the articles
```{r q3 get Ids}
#ids_list <- xml2::as_list(ids)
# Turn the result into a character vector
ids <- as.character(ids)

# Find all the ids 
ids <- stringr::str_extract_all(ids, "<Id>[:digit:]+</Id>")[[1]]

# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
#ids <- stringr::str_remove_all(ids, "<?/Id>")
```

```{r q3 get abstracts}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db = "pubmed",
    id = I(paste(ids, collapse = ",")),
    retmax = 1000,
    rettype = "abstract"
    )
)

# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
```

## Question 4: Distribution of universities, schools, and departments
```{r q4 institution}
institution <- str_extract_all(
  str_to_lower(publications_txt),
  "university\\s+of\\s+(southern|new|northern|the|south|eastern|western)?\\s*[[:alpha:]-]+|[[:alpha:]-]+\\sinstitute\\s+of\\s+[[:alpha:]-]+"
  ) 
institution <- unlist(institution)
table(institution) %>% knitr::kable()

```
```{r q4 schools}
schools_and_deps <- str_extract_all(
  str_to_lower(publications_txt),
  "school\\s+of\\s+[[:alpha:]-]+|department\\s+of\\s+[[:alpha:]-]+"
  )
table(schools_and_deps) %>% knitr::kable()
```

## Question 5: Form a Database
```{r q5 abstracts}
##Extracting all Abstracts from html
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
abstracts <- str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]- =\"]>+")
#replacing newlines or multiple spaces with single space
abstracts <- str_replace_all(abstracts, "[[:space:]]+", " ")
##checking how many abstracts are not provided
table(is.na(abstracts))
```

```{r titles}
##Extracting all titles from html
titles <- str_extract(pub_char_list, "<ArticleTitle>[[:print:][:space:]]+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]-=\"]+>")
#checking how many titles there are
table(is.na(titles))
```
Putting everything into one dataframe `database`, and outputting first 10 rows into a table.
```{r}
database <- data.frame(
  PubMedId = ids,
  Title = titles,
  Abstract = abstracts
)
knitr::kable(database[1:10,], caption = "Some Papers on PubMed about SARS-CoV2 and Hawaii")
```


